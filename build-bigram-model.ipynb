{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives:\n",
    "We need to build medical vocabulary (word dictionary) for spelling checker. The following dataset was found useful for building vocab:(https://www.kaggle.com/datasets/jpmiller/layoutlm/data)\n",
    "\n",
    "Before building the vocab, the following questions needs to be clarified:\n",
    "1. What the dataset contains (EDA)?\n",
    "2. Which feature needed?\n",
    "3. What is the expected outcome?\n",
    "\n",
    "Once, these questions are clarified, we can proceed building a custom NLTK corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import statistics\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "from app_config import Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/medquad-kaggle-johnm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What the dataset contains?\n",
    "We need a corpus that contains medical words for building a medical dictionary (vocab). Therefore, we need to investigate whether the obtained corpus contains the required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate the data types and columns in the dataset.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It appears there are four columns, and all columns contains string datatype.\n",
    "# Let's investigate which column will be more sensible for building the vocab.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function returns token count for given text, it will be used for calculating\n",
    "# average tokens for questions & answers.\n",
    "def token_count(x):\n",
    "    return len(word_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use the word_tokenizer to count the tokens in each column, we need to drop the missing values to avoid exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find out the missing values first.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above we can conclude, question column doesn't have missing values meanwhile, answer column have 5.\n",
    "cnt = df['question'].apply(token_count).sum()\n",
    "print(f'Questions have {cnt} count of tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop only rows with missing values on the answer columns.\n",
    "df = df[df['answer'].notna()]\n",
    "\n",
    "cnt = df['answer'].apply(token_count).sum()\n",
    "print(f'Answers have {cnt} count of tokens.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building Bi-gram model\n",
    "\n",
    "In the process of building bi-gram model, we need to:\n",
    "\n",
    "1. preparing the method to save and load the bi-gram model in pkl file\n",
    "2. clean the text from answer column\n",
    "3. split the paragraph in answer column to sentences\n",
    "4. using nlppreprocess to handle each sentence text preprocessing\n",
    "5. tokenization the sentence after nlppreprocess\n",
    "6. implement padding after tokenization\n",
    "7. using nltk.bigrams to build the bi-gram model\n",
    "8. save the model into pkl file\n",
    "9. verify the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. preparing the method to save and load the bi-gram model in pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "model_path = \"data/bigram_freq.pkl\"\n",
    "tokens = []\n",
    "bigram_freq = defaultdict(Counter)\n",
    "\n",
    "def save_model():\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(bigram_freq, f)\n",
    "    print(f\"Bi-gram saved to {model_path}\")\n",
    "\n",
    "def load_model():\n",
    "    with open(model_path, 'rb') as f:\n",
    "        bigram_freq = pickle.load(f)\n",
    "    return bigram_freq\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    text = \"this is a simple example to demonstrate bigram saving and loading and it's include possesion, I'm Phang Yuen Jun\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    bigram_list = list(bigrams(tokens))\n",
    "    \n",
    "    print(bigram_list)\n",
    "    \n",
    "    for w1, w2 in bigram_list:\n",
    "        bigram_freq[w1][w2] += 1\n",
    "        \n",
    "    save_model()\n",
    "    bigram_freq = load_model()\n",
    "    print(bigram_freq)\n",
    "    \n",
    "    bigram_freq = defaultdict(Counter)\n",
    "    save_model()\n",
    "    bigram_freq = load_model()\n",
    "    print(bigram_freq)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. clean the text from answer column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.regex as rx\n",
    "from importlib import reload\n",
    "\n",
    "reload(rx)\n",
    "# Tracing value for debugging.\n",
    "i = 0\n",
    "clean_text = ''\n",
    "final_text = ''\n",
    "\n",
    "try:\n",
    "    for text in df['answer']:\n",
    "        # Remove URLs.\n",
    "        clean_text = rx.remove_url(text)\n",
    "        # # Remove HTML tags.\n",
    "        clean_text = rx.remove_html(clean_text)\n",
    "        # # Remove bracketed words (usually acronyms).\n",
    "        clean_text = rx.remove_bracketed_text(clean_text)\n",
    "        if final_text == '':\n",
    "            final_text = clean_text\n",
    "        else:\n",
    "            final_text = final_text + ' ' + clean_text\n",
    "        # Tracing row-count for debugging.\n",
    "        i += 1\n",
    "except Exception as e:\n",
    "    print(f'Exception {e} in {i}.')\n",
    "    \n",
    "clean_text = final_text\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = clean_text.split()\n",
    "print(len(s1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. split the paragraph in answer column to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# convert paragraph into sentences\n",
    "sentences = sent_tokenize(clean_text)\n",
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. using nlppreprocess to handle each sentence text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlppreprocess import NLP\n",
    "clean_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.lower()\n",
    "    if not sentence.strip():\n",
    "        clean_sentence = ''\n",
    "    else:\n",
    "        nlp = NLP()\n",
    "        clean_sentence = nlp.process(sentence)\n",
    "        clean_sentences.append(clean_sentence)\n",
    "        \n",
    "print(len(clean_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. tokenization the sentence after nlppreprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "for clean_sentence in clean_sentences:\n",
    "    tokens = word_tokenize(clean_sentence.lower())\n",
    "    print(tokens)\n",
    "\n",
    "    merged_tokens = []\n",
    "    contractions = {\"s\", \"re\", \"m\", \"ll\", \"t\", \"ve\", \"t\"}  # Contractions to merge\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and tokens[i + 1] in contractions:\n",
    "            merged_tokens.append(tokens[i] + \"'\" + tokens[i + 1])  # Merge word + contraction\n",
    "            i += 2  # Skip the next token (contraction)\n",
    "        else:\n",
    "            merged_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "\n",
    "    print(merged_tokens)\n",
    "    tokens = merged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "for clean_sentence in clean_sentences:\n",
    "    total = total + len(tokens)\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. implement padding after tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "for clean_sentence in clean_sentences:\n",
    "    tokens = word_tokenize(clean_sentence.lower())\n",
    "    merged_tokens = []\n",
    "    contractions = {\"s\", \"re\", \"m\", \"ll\", \"t\", \"ve\", \"t\"}  # Contractions to merge\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and tokens[i + 1] in contractions:\n",
    "            merged_tokens.append(tokens[i] + \"'\" + tokens[i + 1])  # Merge word + contraction\n",
    "            i += 2  # Skip the next token (contraction)\n",
    "        else:\n",
    "            merged_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "\n",
    "    print(merged_tokens)\n",
    "    tokens = merged_tokens\n",
    "    # Add padding (start and end symbols)\n",
    "    padded_tokens = [\"<s>\"] + tokens + [\"</s>\"]\n",
    "    print(padded_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. using nltk.bigrams to build the bi-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "bigram_freq = defaultdict(Counter)\n",
    "\n",
    "for clean_sentence in clean_sentences:\n",
    "    tokens = word_tokenize(clean_sentence.lower())\n",
    "    merged_tokens = []\n",
    "    contractions = {\"s\", \"re\", \"m\", \"ll\", \"t\", \"ve\", \"t\"}  # Contractions to merge\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1 and tokens[i + 1] in contractions:\n",
    "            merged_tokens.append(tokens[i] + \"'\" + tokens[i + 1])  # Merge word + contraction\n",
    "            i += 2  # Skip the next token (contraction)\n",
    "        else:\n",
    "            merged_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    tokens = merged_tokens\n",
    "    # Add padding (start and end symbols)\n",
    "    padded_tokens = [\"<s>\"] + tokens + [\"</s>\"]\n",
    "    bigram_list = list(bigrams(padded_tokens))\n",
    "    for w1, w2 in bigram_list:\n",
    "        bigram_freq[w1][w2] += 1\n",
    "        \n",
    "print(bigram_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. save the model into pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. verify the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = load_model()\n",
    "\n",
    "print(\"Loaded Bigram:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the word based on bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(word, top_n=100):\n",
    "    word = word.lower()\n",
    "    if word in bigram_freq:\n",
    "        predictions = bigram_freq[word].most_common(top_n)\n",
    "        return [w for w, _ in predictions]\n",
    "    else:\n",
    "        return [\"No prediction available\"]\n",
    "    \n",
    "\n",
    "input_word = \"damage\"\n",
    "predictions = predict_next(input_word)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_suggestions(previous_word, suggestions):\n",
    "    previous_word = previous_word.lower()\n",
    "    ranking = {}\n",
    "    for key in suggestions:\n",
    "        suggestion = suggestions[key].lower()\n",
    "        rank = bigram_freq.get(previous_word, {}).get(suggestion, 0)  # Avoid KeyError\n",
    "        print(rank)\n",
    "        if rank not in ranking:\n",
    "            ranking[rank] = []\n",
    "        ranking[rank].append(suggestion)\n",
    "    # Sort by frequency in descending order\n",
    "    ranked_suggestions = sorted(ranking.items(), key=lambda x: x[0], reverse=True)\n",
    "    # Flatten sorted suggestions into a dictionary\n",
    "    my_dict = {}\n",
    "    i = 0\n",
    "    for _, words in ranked_suggestions:\n",
    "        for word in words:\n",
    "            my_dict[i] = word\n",
    "            i += 1\n",
    "    return my_dict\n",
    "\n",
    "previous_text = \"damage\"\n",
    "sample_suggest = {0 : \"eye's\"}\n",
    "\n",
    "print(rank_suggestions(previous_text, sample_suggest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_free",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
