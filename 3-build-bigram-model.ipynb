{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives:\n",
    "Previously, we have created two corpus files (medical-corpus and medical-freq) from the same data:(https://www.kaggle.com/datasets/jpmiller/layoutlm/data) \\\\\n",
    "\n",
    "Now, we are going to build b-gram model using the same dataset \\\n",
    "\n",
    "We will skip he EDA like we did previously."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:17:21.946252Z",
     "start_time": "2025-03-20T06:17:21.643721Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nlppreprocess import NLP\n",
    "import utils.regex as rx\n",
    "from importlib import reload\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams\n",
    "from collections import defaultdict, Counter"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:17:22.090543Z",
     "start_time": "2025-03-20T06:17:21.949130Z"
    }
   },
   "source": [
    "df = pd.read_csv('data/medquad-kaggle-johnm.csv')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:17:22.177558Z",
     "start_time": "2025-03-20T06:17:22.175894Z"
    }
   },
   "source": [
    "# The following function returns token count for given text, it will be used for calculating\n",
    "# average tokens for questions & answers.\n",
    "def token_count(x):\n",
    "    return len(word_tokenize(x))"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use the word_tokenizer to count the tokens in each column, we need to drop the missing values to avoid exceptions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:17:32.851618Z",
     "start_time": "2025-03-20T06:17:22.182631Z"
    }
   },
   "source": [
    "# Drop only rows with missing values on the answer columns.\n",
    "df = df[df['answer'].notna()]\n",
    "\n",
    "cnt = df['answer'].apply(token_count).sum()\n",
    "print(f'Answers have {cnt} count of tokens.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers have 3731909 count of tokens.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build Bi-gram model\n",
    "\n",
    "In the process of building bi-gram model, we need to:\n",
    "\n",
    "1. Save and load the bi-gram model in pkl file\n",
    "2. Clean the text from answer column\n",
    "3. Split the paragraph in answer column into sentences\n",
    "4. Use `nlppreprocess` package to handle sentence text preprocessing\n",
    "5. Tokenize the sentence after nlppreprocess\n",
    "6. Implement padding after tokenization\n",
    "7. Using nltk.bigrams to build the bi-gram model\n",
    "8. Save the model into pkl file\n",
    "9. Verify the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 1. Save and load the bi-gram model in pkl file"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:17:32.862157Z",
     "start_time": "2025-03-20T06:17:32.857102Z"
    }
   },
   "source": [
    "model_path = \"data/bigram_freq.pkl\"\n",
    "tokens = []\n",
    "bigram_freq = defaultdict(Counter)\n",
    "\n",
    "\n",
    "def save_model():\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(bigram_freq, f)\n",
    "    print(f\"Bi-gram saved to {model_path}\")\n",
    "\n",
    "\n",
    "def load_model():\n",
    "    with open(model_path, 'rb') as f:\n",
    "        bigram_freq = pickle.load(f)\n",
    "    return bigram_freq\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text = \"this is a simple example to demonstrate bigrams model saving and loading\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "bigram_list = list(bigrams(tokens))\n",
    "\n",
    "print(bigram_list)\n",
    "\n",
    "for w1, w2 in bigram_list:\n",
    "    bigram_freq[w1][w2] += 1\n",
    "\n",
    "save_model()\n",
    "bigram_freq = load_model()\n",
    "print(bigram_freq)\n",
    "\n",
    "bigram_freq = defaultdict(Counter)\n",
    "save_model()\n",
    "bigram_freq = load_model()\n",
    "print(bigram_freq)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('this', 'is'), ('is', 'a'), ('a', 'simple'), ('simple', 'example'), ('example', 'to'), ('to', 'demonstrate'), ('demonstrate', 'bigrams'), ('bigrams', 'model'), ('model', 'saving'), ('saving', 'and'), ('and', 'loading')]\n",
      "Bi-gram saved to data/bigram_freq.pkl\n",
      "defaultdict(<class 'collections.Counter'>, {'this': Counter({'is': 1}), 'is': Counter({'a': 1}), 'a': Counter({'simple': 1}), 'simple': Counter({'example': 1}), 'example': Counter({'to': 1}), 'to': Counter({'demonstrate': 1}), 'demonstrate': Counter({'bigrams': 1}), 'bigrams': Counter({'model': 1}), 'model': Counter({'saving': 1}), 'saving': Counter({'and': 1}), 'and': Counter({'loading': 1})})\n",
      "Bi-gram saved to data/bigram_freq.pkl\n",
      "defaultdict(<class 'collections.Counter'>, {})\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 2. Clean the text from answer column"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:17:52.219522Z",
     "start_time": "2025-03-20T06:17:32.878601Z"
    }
   },
   "source": [
    "reload(rx)\n",
    "# Tracing value for debugging.\n",
    "i = 0\n",
    "clean_text = ''\n",
    "final_text = ''\n",
    "\n",
    "try:\n",
    "    for text in df['answer']:\n",
    "        # Remove URLs.\n",
    "        clean_text = rx.remove_url(text)\n",
    "        # # Remove HTML tags.\n",
    "        clean_text = rx.remove_html(clean_text)\n",
    "        # # Remove bracketed words (usually acronyms).\n",
    "        clean_text = rx.remove_bracketed_text(clean_text)\n",
    "        if final_text == '':\n",
    "            final_text = clean_text\n",
    "        else:\n",
    "            final_text = final_text + ' ' + clean_text\n",
    "        # Tracing row-count for debugging.\n",
    "        i += 1\n",
    "except Exception as e:\n",
    "    print(f'Exception {e} in {i}.')\n",
    "\n",
    "clean_text = final_text"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:17:52.321783Z",
     "start_time": "2025-03-20T06:17:52.225403Z"
    }
   },
   "source": [
    "s1 = clean_text.split()\n",
    "print(len(s1))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3233582\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 3. Split the paragraph in answer column to sentences"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:17:53.958332Z",
     "start_time": "2025-03-20T06:17:52.332755Z"
    }
   },
   "source": [
    "# convert paragraph into sentences\n",
    "sentences = sent_tokenize(clean_text)\n",
    "print(len(sentences))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173050\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Using nlppreprocess to handle each sentence text preprocessing\n",
    "What it does:\n",
    "1. Remove punctuations\n",
    "2. Lemmatize the words"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:17:56.038162Z",
     "start_time": "2025-03-20T06:17:53.964475Z"
    }
   },
   "source": [
    "clean_sentences = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    sentence = sentence.lower()\n",
    "    if not sentence.strip():\n",
    "        clean_sentence = ''\n",
    "    else:\n",
    "        nlp = NLP()\n",
    "        clean_sentence = nlp.process(sentence)\n",
    "        clean_sentences.append(clean_sentence)\n",
    "\n",
    "print(len(clean_sentences))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173050\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 5. Tokenize the sentence after nlppreprocess"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:18:04.399121Z",
     "start_time": "2025-03-20T06:17:56.044115Z"
    }
   },
   "source": [
    "for clean_sentence in clean_sentences:\n",
    "    tokens = word_tokenize(clean_sentence.lower())\n",
    "    # clean token when possession\n",
    "    clean_tokens = []\n",
    "    contractions = {\"s\", \"re\", \"m\", \"ll\", \"t\", \"ve\", \"t\"}\n",
    "\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if tokens[i] not in contractions:\n",
    "            clean_tokens.append(tokens[i])\n",
    "        i += 1\n",
    "\n",
    "    tokens = clean_tokens\n",
    "\n",
    "total = 0\n",
    "for clean_sentence in clean_sentences:\n",
    "    total = total + len(tokens)\n",
    "\n",
    "print(total)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1557450\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 6. Implement padding after tokenization"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:18:12.800320Z",
     "start_time": "2025-03-20T06:18:04.405420Z"
    }
   },
   "source": [
    "for clean_sentence in clean_sentences:\n",
    "    tokens = word_tokenize(clean_sentence.lower())\n",
    "    # clean token when possession\n",
    "    clean_tokens = []\n",
    "    contractions = {\"s\", \"re\", \"m\", \"ll\", \"t\", \"ve\"}\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if tokens[i] not in contractions:\n",
    "            clean_tokens.append(tokens[i])\n",
    "        i += 1\n",
    "    tokens = clean_tokens\n",
    "    # Add padding (start and end symbols)\n",
    "    padded_tokens = [\"<s>\"] + tokens + [\"</s>\"]\n",
    "    # print(padded_tokens)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 7. Use nltk.bigrams to build the bi-gram model"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:18:22.777691Z",
     "start_time": "2025-03-20T06:18:12.806298Z"
    }
   },
   "source": [
    "bigram_freq = defaultdict(Counter)\n",
    "\n",
    "for clean_sentence in clean_sentences:\n",
    "    tokens = word_tokenize(clean_sentence.lower())\n",
    "    # clean token when possession\n",
    "    clean_tokens = []\n",
    "    contractions = {\"s\", \"re\", \"m\", \"ll\", \"t\", \"ve\"}\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if tokens[i] not in contractions:\n",
    "            clean_tokens.append(tokens[i])\n",
    "        i += 1\n",
    "    tokens = clean_tokens\n",
    "    # Add padding (start and end symbols)\n",
    "    padded_tokens = [\"<s>\"] + tokens + [\"</s>\"]\n",
    "    bigram_list = list(bigrams(padded_tokens))\n",
    "    for w1, w2 in bigram_list:\n",
    "        bigram_freq[w1][w2] += 1\n",
    "\n",
    "# print(bigram_freq)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 8. Save the model into pkl file"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:18:23.048770Z",
     "start_time": "2025-03-20T06:18:22.904690Z"
    }
   },
   "source": [
    "save_model()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-gram saved to data/bigram_freq.pkl\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 9. Verify the model"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:18:23.300484Z",
     "start_time": "2025-03-20T06:18:23.170436Z"
    }
   },
   "source": [
    "result = load_model()\n",
    "\n",
    "print(\"Loaded Bigram:\")\n",
    "# print(result)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Bigram:\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the word based on bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:18:23.530410Z",
     "start_time": "2025-03-20T06:18:23.527600Z"
    }
   },
   "source": [
    "def predict_next(word, top_n=10):\n",
    "    word = word.lower()\n",
    "    if word in bigram_freq:\n",
    "        predictions = bigram_freq[word].most_common(top_n)\n",
    "        return [w for w, _ in predictions]\n",
    "    else:\n",
    "        return [\"No prediction available\"]\n",
    "\n",
    "\n",
    "input_word = \"damage\"\n",
    "predictions = predict_next(input_word)\n",
    "\n",
    "print(predictions)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', 'and', 'your', 'in', 'brain', 'heart', 'can', 'from', 'liver', 'caused']\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T06:18:23.763796Z",
     "start_time": "2025-03-20T06:18:23.760321Z"
    }
   },
   "source": [
    "def rank_suggestions(previous_word, suggestions):\n",
    "    previous_word = previous_word.lower()\n",
    "    ranking = {}\n",
    "    for key in suggestions:\n",
    "        suggestion = suggestions[key].lower()\n",
    "        rank = bigram_freq.get(previous_word, {}).get(suggestion, 0)  # Avoid KeyError\n",
    "        print(rank)\n",
    "        if rank not in ranking:\n",
    "            ranking[rank] = []\n",
    "        ranking[rank].append(suggestion)\n",
    "    # Sort by frequency in descending order\n",
    "    ranked_suggestions = sorted(ranking.items(), key=lambda x: x[0], reverse=True)\n",
    "    # Flatten sorted suggestions into a dictionary\n",
    "    my_dict = {}\n",
    "    i = 0\n",
    "    for _, words in ranked_suggestions:\n",
    "        for word in words:\n",
    "            my_dict[i] = word\n",
    "            i += 1\n",
    "    return my_dict\n",
    "\n",
    "\n",
    "previous_text = \"damage\"\n",
    "sample_suggest = {0: \"eye\"}\n",
    "\n",
    "print(rank_suggestions(previous_text, sample_suggest))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "{0: 'eye'}\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_free",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
