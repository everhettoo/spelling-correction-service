{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f363de6c0adfb5",
   "metadata": {},
   "source": [
    "# Objectives:\n",
    "We have created the vocab previously. Now for noisy channel we need to create a vocab file with frequency in TSV format. \\\n",
    "Same source is used to build the file -> (https://www.kaggle.com/datasets/jpmiller/layoutlm/data)\n",
    "\n",
    "Since,we have done EDA on the previous stage (while creating corpus), we will skip EDA and jump into pre-processing and vocab-frequency file creation."
   ]
  },
  {
   "cell_type": "code",
   "id": "3b42496ad2ad5d83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:10:52.840093Z",
     "start_time": "2025-03-17T14:10:52.837976Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import statistics\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "from app_config import Configuration"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "id": "f68d7f5191a962eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:10:53.024366Z",
     "start_time": "2025-03-17T14:10:52.863522Z"
    }
   },
   "source": [
    "df = pd.read_csv('data/medquad-kaggle-johnm.csv')"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:10:53.029406Z",
     "start_time": "2025-03-17T14:10:53.027493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The following function returns token count for given text, it will be used for calculating\n",
    "# average tokens for questions & answers.\n",
    "def token_count(x):\n",
    "    return len(word_tokenize(x))"
   ],
   "id": "20e3b58934f9c708",
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "id": "1576b8c0aa210b03",
   "metadata": {},
   "source": [
    "Before we use the word_tokenizer to count the tokens in each column, we need to drop the missing values to avoid exceptions."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:11:04.343222Z",
     "start_time": "2025-03-17T14:10:53.046695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Drop only rows with missing values on the answer columns.\n",
    "df = df[df['answer'].notna()]\n",
    "\n",
    "cnt = df['answer'].apply(token_count).sum()\n",
    "print(f'Answers have {cnt} count of tokens.')"
   ],
   "id": "e8e9dbb9cec7360b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers have 3731909 count of tokens.\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### In the preprocessing to build the vocab, we need to:\n",
    "1. Clean the text from answer column - only words (others are stripped)\n",
    "2. Get lemma for each words to avoid redundant word with the same meaning\n",
    "3. Only include unique words into vocab"
   ],
   "id": "d9f859c57df3994c"
  },
  {
   "cell_type": "code",
   "id": "2c5e211a1e06408f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:11:22.523567Z",
     "start_time": "2025-03-17T14:11:04.407477Z"
    }
   },
   "source": [
    "from importlib import reload\n",
    "# 1. Clean the text from answer column - only words (others are stripped)\n",
    "from nltk.corpus import stopwords\n",
    "import utils.regex as rx\n",
    "\n",
    "reload(rx)\n",
    "\n",
    "# Acquire the stop words from NLTK corpus.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# To store all the processed tokens.\n",
    "corpus_token_list = []\n",
    "\n",
    "# Tracing value for debugging.\n",
    "i = 0\n",
    "token_cnt = 0\n",
    "filtered_token_cnt = 0\n",
    "cleanse_data = pd.DataFrame(columns=['row', 'original', 'cleansed'])\n",
    "try:\n",
    "    for text in df['answer']:\n",
    "        # Remove URLs.\n",
    "        clean_text = rx.remove_url(text)\n",
    "        # # Remove HTML tags.\n",
    "        clean_text = rx.remove_html(clean_text)\n",
    "        # # Remove bracketed words (usually acronyms).\n",
    "        clean_text = rx.remove_bracketed_text(clean_text)\n",
    "        # Transform contradictions to full form first before removing stop words.\n",
    "        clean_text = rx.transform_contractions(clean_text)\n",
    "        # Get only words.\n",
    "        clean_text = rx.get_words(clean_text.lower())\n",
    "        # Remove all extra spaces.\n",
    "        clean_text = rx.remove_extra_space(clean_text)\n",
    "        # For tracing raw to cleanse.\n",
    "        cleanse_data.loc[len(cleanse_data)] = [i, text, clean_text]\n",
    "        # Tokenize the text.\n",
    "        tokens = word_tokenize(clean_text)\n",
    "        # Tracing unfiltered-token count.\n",
    "        token_cnt += len(tokens)\n",
    "        # Filter stop words.\n",
    "        filtered_text = [w for w in tokens if not w.lower() in stop_words]\n",
    "        # Tracing filtered-token count for debugging.\n",
    "        filtered_token_cnt += len(filtered_text)\n",
    "        # Add the filtered words into corpus token_list.\n",
    "        corpus_token_list.append(filtered_text)\n",
    "        # Tracing row-count for debugging.\n",
    "        i += 1\n",
    "except Exception as e:\n",
    "    print(f'Exception {e} in {i}.')\n",
    "\n",
    "print(f'Rows processed:[{i}], unfiltered tokens:[{token_cnt}], filtered tokens:[{filtered_token_cnt}]')\n",
    "print(f'Corpus entry count:[{len(corpus_token_list)}].')\n",
    "\n",
    "cleanse_data.to_csv('data/cleanse-data-freq.csv', index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows processed:[16407], unfiltered tokens:[3151455], filtered tokens:[1885840]\n",
      "Corpus entry count:[16407].\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "id": "f1d64c170f9a3dd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:11:22.547850Z",
     "start_time": "2025-03-17T14:11:22.543884Z"
    }
   },
   "source": [
    "cleanse_data.iloc[580:590]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     row                                           original  \\\n",
       "580  580  Here are links to more information about P.A.D...   \n",
       "581  581  Many Reasons for Abuse Drug abuse, whether pre...   \n",
       "582  582  Addiction is a chronic disease in which a pers...   \n",
       "583  583  Physical dependence is a normal process that c...   \n",
       "584  584  A persons behavior, especially changes in beha...   \n",
       "585  585  The prescription medications most commonly abu...   \n",
       "586  586  Medications affect older people differently th...   \n",
       "587  587  Marijuana is the most abused illicit drug amon...   \n",
       "588  588  Although under federal law, marijuana is illeg...   \n",
       "589  589  Not always. Some warning signs, such as sleep ...   \n",
       "\n",
       "                                              cleansed  \n",
       "580  here are links to more information about p a d...  \n",
       "581  many reasons for abuse drug abuse whether pres...  \n",
       "582  addiction is a chronic disease in which a pers...  \n",
       "583  physical dependence is a normal process that c...  \n",
       "584  a persons behavior especially changes in behav...  \n",
       "585  the prescription medications most commonly abu...  \n",
       "586  medications affect older people differently th...  \n",
       "587  marijuana is the most abused illicit drug amon...  \n",
       "588  although under federal law marijuana is illega...  \n",
       "589  not always some warning signs such as sleep pr...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row</th>\n",
       "      <th>original</th>\n",
       "      <th>cleansed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>580</td>\n",
       "      <td>Here are links to more information about P.A.D...</td>\n",
       "      <td>here are links to more information about p a d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>581</td>\n",
       "      <td>Many Reasons for Abuse Drug abuse, whether pre...</td>\n",
       "      <td>many reasons for abuse drug abuse whether pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>582</td>\n",
       "      <td>Addiction is a chronic disease in which a pers...</td>\n",
       "      <td>addiction is a chronic disease in which a pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>583</td>\n",
       "      <td>Physical dependence is a normal process that c...</td>\n",
       "      <td>physical dependence is a normal process that c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>584</td>\n",
       "      <td>A persons behavior, especially changes in beha...</td>\n",
       "      <td>a persons behavior especially changes in behav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>585</td>\n",
       "      <td>The prescription medications most commonly abu...</td>\n",
       "      <td>the prescription medications most commonly abu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>586</td>\n",
       "      <td>Medications affect older people differently th...</td>\n",
       "      <td>medications affect older people differently th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>587</td>\n",
       "      <td>Marijuana is the most abused illicit drug amon...</td>\n",
       "      <td>marijuana is the most abused illicit drug amon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>588</td>\n",
       "      <td>Although under federal law, marijuana is illeg...</td>\n",
       "      <td>although under federal law marijuana is illega...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>589</td>\n",
       "      <td>Not always. Some warning signs, such as sleep ...</td>\n",
       "      <td>not always some warning signs such as sleep pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "id": "ed7e2093a951bc5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:11:29.145537Z",
     "start_time": "2025-03-17T14:11:22.585061Z"
    }
   },
   "source": [
    "# TODO: 23-03-2025: Wrong word 'dressingsthese' appeared in the medical.txt.  This is GIGO.\n",
    "\n",
    "# 2. Get lemma for each words to avoid redundant word with the same meaning\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# List for storing token's lemma.\n",
    "corpus_lemma_list = []\n",
    "\n",
    "# Tracing value for debugging.\n",
    "i = 0\n",
    "token_cnt = 0\n",
    "filtered_token_cnt = 0\n",
    "# normalized_data = pd.DataFrame(columns=['row', 'original', 'normalized'])\n",
    "trace_list = []\n",
    "for entry in corpus_token_list:\n",
    "    entry_list = []\n",
    "    for token in entry:\n",
    "        normalized_token = lemmatizer.lemmatize(token)\n",
    "        entry_list.append(normalized_token)\n",
    "\n",
    "        # For tracing raw to cleanse.\n",
    "        trace_list.append([i, token, normalized_token])\n",
    "        # normalized_data.loc[len(normalized_data)] = [i, token, normalized_token]\n",
    "\n",
    "        filtered_token_cnt += 1\n",
    "\n",
    "    # Add entry (token for single answer) in the list for further processing.\n",
    "    corpus_lemma_list.append(entry_list)\n",
    "    i = i + 1\n",
    "\n",
    "print(f'Rows processed:[{i}], filtered tokens:[{filtered_token_cnt}]')\n",
    "print(f'Corpus entry count:[{len(corpus_token_list)}].')\n",
    "\n",
    "normalized_data = pd.DataFrame(trace_list, columns=['row', 'original', 'normalized'])\n",
    "normalized_data.to_csv('data/normalized-data-freq.csv', index=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows processed:[16407], filtered tokens:[1885840]\n",
      "Corpus entry count:[16407].\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "4206fb9d61a5cc50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:11:29.172229Z",
     "start_time": "2025-03-17T14:11:29.168283Z"
    }
   },
   "source": [
    "normalized_data.iloc[1780:1790]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      row    original normalized\n",
       "1780   15    pressure   pressure\n",
       "1781   15      causes      cause\n",
       "1782   15      causes      cause\n",
       "1783   15        high       high\n",
       "1784   15       blood      blood\n",
       "1785   15    pressure   pressure\n",
       "1786   15     include    include\n",
       "1787   15     medical    medical\n",
       "1788   15  conditions  condition\n",
       "1789   15     chronic    chronic"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row</th>\n",
       "      <th>original</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>15</td>\n",
       "      <td>pressure</td>\n",
       "      <td>pressure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>15</td>\n",
       "      <td>causes</td>\n",
       "      <td>cause</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>15</td>\n",
       "      <td>causes</td>\n",
       "      <td>cause</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>15</td>\n",
       "      <td>high</td>\n",
       "      <td>high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>15</td>\n",
       "      <td>blood</td>\n",
       "      <td>blood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>15</td>\n",
       "      <td>pressure</td>\n",
       "      <td>pressure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>15</td>\n",
       "      <td>include</td>\n",
       "      <td>include</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>15</td>\n",
       "      <td>medical</td>\n",
       "      <td>medical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>15</td>\n",
       "      <td>conditions</td>\n",
       "      <td>condition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>15</td>\n",
       "      <td>chronic</td>\n",
       "      <td>chronic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "id": "5b31d9460626042c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:11:29.261164Z",
     "start_time": "2025-03-17T14:11:29.203629Z"
    }
   },
   "source": [
    "# Upon random checking on the vocab, 'dressingsthese' was found. Further investigation was conducted to trace the outcome.\n",
    "normalized_data[normalized_data['normalized'] == 'dressingsthese']"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      row        original      normalized\n",
       "5141   32  dressingsthese  dressingsthese\n",
       "5235   32  dressingsthese  dressingsthese"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row</th>\n",
       "      <th>original</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5141</th>\n",
       "      <td>32</td>\n",
       "      <td>dressingsthese</td>\n",
       "      <td>dressingsthese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5235</th>\n",
       "      <td>32</td>\n",
       "      <td>dressingsthese</td>\n",
       "      <td>dressingsthese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "e1c15bd8f4ea6371",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:11:29.519503Z",
     "start_time": "2025-03-17T14:11:29.301783Z"
    }
   },
   "source": [
    "# 3. Only include unique words into vocab\n",
    "\n",
    "# Final vocab to store the lemmas from the corpus.\n",
    "from collections import Counter\n",
    "\n",
    "vocab_list = []\n",
    "corpus_lemma_list = sorted(corpus_lemma_list, key=lambda s: (len(s), s))\n",
    "# Count occurrences of words\n",
    "word_counts = Counter(word for entry in corpus_lemma_list for word in entry)\n",
    "# Convert to a list of objects (dictionaries)\n",
    "vocab_list = [{\"text\": word, \"freq\": count} for word, count in word_counts.items()]\n",
    "\n",
    "# Sort alphabetically by text\n",
    "vocab_list.sort(key=lambda x: (len(x[\"text\"]), x[\"text\"]))\n",
    "\n",
    "print(f'Vocab entry count (unique words):[{len(vocab_list)}].')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab entry count (unique words):[23319].\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "id": "65a319105175033e",
   "metadata": {},
   "source": "### Now, we have the unique medical words stored in vocab and ready for creating a frequency corpus for noisy-channel."
  },
  {
   "cell_type": "code",
   "id": "ffe8c9b32661736f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:11:29.565907Z",
     "start_time": "2025-03-17T14:11:29.540968Z"
    }
   },
   "source": [
    "# Save the vocab into custom NLTK corpus format.\n",
    "\n",
    "import os\n",
    "\n",
    "# Get corpus path from app config.\n",
    "config = Configuration()\n",
    "\n",
    "# Organize the vocab into custom folder.\n",
    "corpus_dir = config.config_values['corpus_medical_freq_dir']\n",
    "if not os.path.exists(corpus_dir):\n",
    "    os.makedirs(corpus_dir)\n",
    "\n",
    "corpus_name = config.config_values['corpus_medical_freq_name']\n",
    "\n",
    "df = pd.DataFrame(vocab_list)\n",
    "df.tail()\n",
    "df.to_csv(corpus_dir + \"/\" + corpus_name, sep=\"\\t\", index=False, header=None)"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "id": "99feedb22a58d5d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T14:11:29.613212Z",
     "start_time": "2025-03-17T14:11:29.569389Z"
    }
   },
   "source": [
    "# Load the custom NLTK corpus.\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "# Step 3: Create an NLTK Corpus Reader\n",
    "corpus = PlaintextCorpusReader(corpus_dir, '.*\\.tsv')\n",
    "\n",
    "print(f'There are {len(corpus.words())} words in custom corpus.')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 46638 words in custom corpus.\n"
     ]
    }
   ],
   "execution_count": 54
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_free",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
