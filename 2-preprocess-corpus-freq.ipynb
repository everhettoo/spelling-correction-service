{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f363de6c0adfb5",
   "metadata": {},
   "source": [
    "# Objectives:\n",
    "We need to build medical vocabulary (word dictionary) for spelling checker. The following dataset was found useful for building vocab:(https://www.kaggle.com/datasets/jpmiller/layoutlm/data)\n",
    "\n",
    "Before building the vocab, the following questions needs to be clarified:\n",
    "1. What the dataset contains (EDA)?\n",
    "2. Which feature needed?\n",
    "3. What is the expected outcome?\n",
    "\n",
    "Once, these questions are clarified, we can proceed building a custom corpus for noisy-channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42496ad2ad5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import statistics\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "from app_config import Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d7f5191a962eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/medquad-kaggle-johnm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b9493657efbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad211965e39f4dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e9f70a93c7f2c9",
   "metadata": {},
   "source": [
    "## 1. What the dataset contains?\n",
    "We need a corpus that contains medical words for building a medical dictionary (vocab). Therefore, we need to investigate whether the obtained corpus contains the required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a82c826d392f08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:25:59.419462Z",
     "start_time": "2025-03-15T09:25:59.413512Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's investigate the data types and columns in the dataset.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95daddeb8d732d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:26:01.224893Z",
     "start_time": "2025-03-15T09:26:01.199747Z"
    }
   },
   "outputs": [],
   "source": [
    "# It appears there are four columns, and all columns contains string datatype.\n",
    "# Let's investigate which column will be more sensible for building the vocab.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1391d9d1f0e8512",
   "metadata": {},
   "source": [
    "Logically assumption: average length of answers should be greater than questions; means, more texts in answers column. \\\n",
    "Let's investigate the assumption.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e3b58934f9c708",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:26:03.726159Z",
     "start_time": "2025-03-15T09:26:03.724299Z"
    }
   },
   "outputs": [],
   "source": [
    "# The following function returns token count for given text, it will be used for calculating\n",
    "# average tokens for questions & answers.\n",
    "def token_count(x):\n",
    "    return len(word_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576b8c0aa210b03",
   "metadata": {},
   "source": [
    "Before we use the word_tokenizer to count the tokens in each column, we need to drop the missing values to avoid exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76713f489bacd3b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:26:06.525962Z",
     "start_time": "2025-03-15T09:26:06.520999Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's find out the missing values first.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0960dd56836b97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:26:10.857691Z",
     "start_time": "2025-03-15T09:26:10.259537Z"
    }
   },
   "outputs": [],
   "source": [
    "# From the above we can conclude, question column doesn't have missing values meanwhile, answer column have 5.\n",
    "cnt = df['question'].apply(token_count).sum()\n",
    "print(f'Questions have {cnt} count of tokens.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e9dbb9cec7360b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:26:24.811333Z",
     "start_time": "2025-03-15T09:26:13.469206Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop only rows with missing values on the answer columns.\n",
    "df = df[df['answer'].notna()]\n",
    "\n",
    "cnt = df['answer'].apply(token_count).sum()\n",
    "print(f'Answers have {cnt} count of tokens.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f859c57df3994c",
   "metadata": {},
   "source": [
    "## 2. Which feature needed?\n",
    "In the EDA, two columns (question & answer) from the dataset was expected to have the required texts to build the vocab. For that, an hypothesis made that the answer column would have longer text than question column. The hypothesis was true, and therefore, the text from the answer column will be preprocessed for building the corpus.\\\n",
    "\n",
    "In the preprocessing to build the vocab, we need to:\n",
    "1. Clean the text from answer column - only words (others are stripped)\n",
    "2. Get lemma for each words to avoid redundant word with the same meaning\n",
    "3. Only include unique words into vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e211a1e06408f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:27:02.300774Z",
     "start_time": "2025-03-15T09:26:44.067477Z"
    }
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "# 1. Clean the text from answer column - only words (others are stripped)\n",
    "from nltk.corpus import stopwords\n",
    "import utils.regex as rx\n",
    "\n",
    "reload(rx)\n",
    "\n",
    "# Acquire the stop words from NLTK corpus.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# To store all the processed tokens.\n",
    "corpus_token_list = []\n",
    "\n",
    "# Tracing value for debugging.\n",
    "i = 0\n",
    "token_cnt = 0\n",
    "filtered_token_cnt = 0\n",
    "cleanse_data = pd.DataFrame(columns=['row', 'original', 'cleansed'])\n",
    "try:\n",
    "    for text in df['answer']:\n",
    "        # Remove URLs.\n",
    "        clean_text = rx.remove_url(text)\n",
    "        # # Remove HTML tags.\n",
    "        clean_text = rx.remove_html(clean_text)\n",
    "        # # Remove bracketed words (usually acronyms).\n",
    "        clean_text = rx.remove_bracketed_text(clean_text)\n",
    "        # Transform contradictions to full form first before removing stop words.\n",
    "        clean_text = rx.transform_contractions(clean_text)\n",
    "        # Get only words.\n",
    "        clean_text = rx.get_words(clean_text.lower())\n",
    "        # Remove all extra spaces.\n",
    "        clean_text = rx.remove_extra_space(clean_text)\n",
    "        # For tracing raw to cleanse.\n",
    "        cleanse_data.loc[len(cleanse_data)] = [i, text, clean_text]\n",
    "        # Tokenize the text.\n",
    "        tokens = word_tokenize(clean_text)\n",
    "        # Tracing unfiltered-token count.\n",
    "        token_cnt += len(tokens)\n",
    "        # Filter stop words.\n",
    "        filtered_text = [w for w in tokens if not w.lower() in stop_words]\n",
    "        # Tracing filtered-token count for debugging.\n",
    "        filtered_token_cnt += len(filtered_text)\n",
    "        # Add the filtered words into corpus token_list.\n",
    "        corpus_token_list.append(filtered_text)\n",
    "        # Tracing row-count for debugging.\n",
    "        i += 1\n",
    "except Exception as e:\n",
    "    print(f'Exception {e} in {i}.')\n",
    "\n",
    "print(f'Rows processed:[{i}], unfiltered tokens:[{token_cnt}], filtered tokens:[{filtered_token_cnt}]')\n",
    "print(f'Corpus entry count:[{len(corpus_token_list)}].')\n",
    "\n",
    "cleanse_data.to_csv('data/cleanse-data-freq.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d64c170f9a3dd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:27:09.489754Z",
     "start_time": "2025-03-15T09:27:09.484396Z"
    }
   },
   "outputs": [],
   "source": [
    "cleanse_data.iloc[580:590]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7e2093a951bc5e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:27:27.745967Z",
     "start_time": "2025-03-15T09:27:21.487967Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: 23-03-2025: Wrong word 'dressingsthese' appeared in the medical.txt.  This is GIGO.\n",
    "\n",
    "# 2. Get lemma for each words to avoid redundant word with the same meaning\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# List for storing token's lemma.\n",
    "corpus_lemma_list = []\n",
    "\n",
    "# Tracing value for debugging.\n",
    "i = 0\n",
    "token_cnt = 0\n",
    "filtered_token_cnt = 0\n",
    "# normalized_data = pd.DataFrame(columns=['row', 'original', 'normalized'])\n",
    "trace_list = []\n",
    "for entry in corpus_token_list:\n",
    "    entry_list = []\n",
    "    for token in entry:\n",
    "        normalized_token = lemmatizer.lemmatize(token)\n",
    "        entry_list.append(normalized_token)\n",
    "\n",
    "        # For tracing raw to cleanse.\n",
    "        trace_list.append([i, token, normalized_token])\n",
    "        # normalized_data.loc[len(normalized_data)] = [i, token, normalized_token]\n",
    "\n",
    "        filtered_token_cnt += 1\n",
    "\n",
    "    # Add entry (token for single answer) in the list for further processing.\n",
    "    corpus_lemma_list.append(entry_list)\n",
    "    i = i + 1\n",
    "\n",
    "print(f'Rows processed:[{i}], filtered tokens:[{filtered_token_cnt}]')\n",
    "print(f'Corpus entry count:[{len(corpus_token_list)}].')\n",
    "\n",
    "normalized_data = pd.DataFrame(trace_list, columns=['row', 'original', 'normalized'])\n",
    "normalized_data.to_csv('data/normalized-data-freq.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4206fb9d61a5cc50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:27:32.860213Z",
     "start_time": "2025-03-15T09:27:32.855991Z"
    }
   },
   "outputs": [],
   "source": [
    "normalized_data.iloc[1780:1790]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31d9460626042c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:27:38.533258Z",
     "start_time": "2025-03-15T09:27:38.475599Z"
    }
   },
   "outputs": [],
   "source": [
    "# Upon random checking on the vocab, 'dressingsthese' was found. Further investigation was conducted to trace the outcome.\n",
    "normalized_data[normalized_data['normalized'] == 'dressingsthese']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b336e023991b943a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:27:45.977225Z",
     "start_time": "2025-03-15T09:27:45.974638Z"
    }
   },
   "outputs": [],
   "source": [
    "# The above indicates that the 'dressingsthese' word originated from the source. Let's find out with the row-id=32 from the source data.\n",
    "s3 = df.iloc[32:33]['answer'].tolist()\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14aab75d908351a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:27:49.160034Z",
     "start_time": "2025-03-15T09:27:49.157405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Upon investigation this was found to be GIGO.\n",
    "s1 = cleanse_data[cleanse_data['row'] == 32]['original'].tolist()\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd16621b4165285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:27:55.605662Z",
     "start_time": "2025-03-15T09:27:51.539364Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's see the most frequent words in the vocab.\n",
    "import wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = ''\n",
    "for entry in corpus_lemma_list:\n",
    "    for token in entry:\n",
    "        text += ' ' + token\n",
    "\n",
    "word_cloud = wordcloud.WordCloud(background_color='white').generate(text)\n",
    "plt.figure(figsize=(15, 8), facecolor=None)\n",
    "plt.imshow(word_cloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c15bd8f4ea6371",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:28:27.665847Z",
     "start_time": "2025-03-15T09:28:27.443828Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Only include unique words into vocab\n",
    "\n",
    "# Final vocab to store the lemmas from the corpus.\n",
    "from collections import Counter\n",
    "\n",
    "vocab_list = []\n",
    "corpus_lemma_list = sorted(corpus_lemma_list, key=lambda s: (len(s), s))\n",
    "# Count occurrences of words\n",
    "word_counts = Counter(word for entry in corpus_lemma_list for word in entry)\n",
    "# Convert to a list of objects (dictionaries)\n",
    "vocab_list = [{\"text\": word, \"freq\": count} for word, count in word_counts.items()]\n",
    "\n",
    "# Sort alphabetically by text\n",
    "vocab_list.sort(key=lambda x: (len(x[\"text\"]), x[\"text\"]))\n",
    "\n",
    "print(f'Vocab entry count (unique words):[{len(vocab_list)}].')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a319105175033e",
   "metadata": {},
   "source": [
    "## 3. What is the expected outcome?\n",
    "Now, we have the unique medical words stored in vocab and ready for creating a frequency corpus for noisy-channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8c9b32661736f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:29:47.538656Z",
     "start_time": "2025-03-15T09:29:47.506466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the vocab into custom NLTK corpus format.\n",
    "\n",
    "import os\n",
    "\n",
    "# Get corpus path from app config.\n",
    "config = Configuration()\n",
    "\n",
    "# Organize the vocab into custom folder.\n",
    "corpus_dir = config.config_values['corpus_medical_freq_dir']\n",
    "if not os.path.exists(corpus_dir):\n",
    "    os.makedirs(corpus_dir)\n",
    "\n",
    "corpus_name = config.config_values['corpus_medical_freq_name']\n",
    "\n",
    "df = pd.DataFrame(vocab_list)\n",
    "df.tail()\n",
    "df.to_csv(corpus_dir + \"/\" + corpus_name, sep=\"\\t\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99feedb22a58d5d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:30:04.882406Z",
     "start_time": "2025-03-15T09:30:04.836710Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the custom NLTK corpus.\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "# Step 3: Create an NLTK Corpus Reader\n",
    "corpus = PlaintextCorpusReader(corpus_dir, '.*\\.tsv')\n",
    "\n",
    "print(f'There are {len(corpus.words())} words in custom corpus.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3658080d8b69389c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:30:10.883882Z",
     "start_time": "2025-03-15T09:30:10.503487Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing: load the corpus and perform edit-distance for given real-word.\n",
    "from nltk import edit_distance\n",
    "\n",
    "test_word = 'glacoma'\n",
    "\n",
    "if test_word not in corpus.words():\n",
    "    print(f'Test word {test_word} not in corpus.')\n",
    "    for w in corpus.words():\n",
    "        m = edit_distance(test_word, w)\n",
    "        if m == 1:\n",
    "            print(f'Word {w} edit distance is {m}.')\n",
    "            break\n",
    "else:\n",
    "    print(f'Test word {test_word} exists in corpus.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_free",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
